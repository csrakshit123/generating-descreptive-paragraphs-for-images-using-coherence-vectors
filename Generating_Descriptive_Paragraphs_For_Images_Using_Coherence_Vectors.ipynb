{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Descriptive Paragraphs For Images Using Coherence Vectors.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTqNuj3td3Jd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRI1nkded5kA",
        "colab_type": "text"
      },
      "source": [
        "      Rakshit C S, Dept of ECE, PES University\n",
        "    Sirajahamed N D, Dept of CSE, PES University \n",
        "                       and \n",
        "    Prof Rama Devi P, Dept of CSE, PES University"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfO4fx4ldki5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dbcbb72-5de6-450e-ce90-b5d8a3e25452"
      },
      "source": [
        "#Import requires dependancies\n",
        "import json\n",
        "from pickle import load, dump\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import linalg as LA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from timeit import default_timer as timer\n",
        "import sys\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from timeit import default_timer as timer\n",
        "import pickle\n",
        "import string\n",
        "from os import listdir\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpQJztIXe4q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(directory):\n",
        "    # load the model\n",
        "    model = InceptionV3()\n",
        "    # re-structure the model\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "    # extract features from each photo\n",
        "    features = dict()\n",
        "    i = 0\n",
        "    for name in listdir(directory):\n",
        "        # load an image from file\n",
        "        i+=1\n",
        "        print(\"image number : \",i)\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size=(299, 299))\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        # reshape data for the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        # prepare the image for the VGG model\n",
        "        image = preprocess_input(image)\n",
        "        # get features\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        # get image id\n",
        "        image_id = name.split('.')[0]\n",
        "        # store feature\n",
        "        features[image_id] = feature\n",
        "        print(name)\n",
        "    return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJJGQYq7e7hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = 'images directory'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: ', len(features))\n",
        "# save to file\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPY7NovfreG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def load_descriptions(doc,dataset):\n",
        "    mapping = dict()\n",
        "    # process lines\n",
        "    doc = doc.split('\\n')\n",
        "    doc.pop(0)\n",
        "    doc = '\\n'.join(doc)\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split(';')\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        # take the first token as the image id, the rest as the description\n",
        "        image_id , image_desc = tokens[1] , tokens[2:]\n",
        "        if(int(image_id) in dataset):\n",
        "        # convert description tokens back to string\n",
        "             image_desc = ' '.join(image_desc)\n",
        "        # create the list if needed\n",
        "        # store description\n",
        "             mapping[image_id] = image_desc.split('.')\n",
        "             mapping[image_id].pop()\n",
        "    return mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "    # prepare translation table for removing punctuation\n",
        "    x = list(string.punctuation)\n",
        "    x.remove(',')\n",
        "    x = ''.join(x)\n",
        "    \n",
        "    table = str.maketrans('', '', x)\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            # tokenize\n",
        "            desc = desc.split()\n",
        "            # convert to lower case\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # remove punctuation from each token\n",
        "            desc = [w.translate(table) for w in desc]\n",
        "            # remove hanging 's' and 'a'\n",
        "            desc = [word for word in desc if len(word)>1 or (',' in word)]\n",
        "            # remove tokens with numbers in them\n",
        "            desc = [word for word in desc if word.isalpha() or (',' in word) ] \n",
        "            # store as string\n",
        "            desc_list[i] =  ' '.join(desc)\n",
        "\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        # split id from description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # skip images not in the set\n",
        "        if int(image_id) in dataset:\n",
        "            # create list\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            # wrap description in tokens\n",
        "            desc = ' '.join(image_desc)\n",
        "            # store\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def create_sequences(tokenizer, descriptions):\n",
        "    new_desc_list = dict()\n",
        "    # walk through each image identifier\n",
        "    for key, desc_list in descriptions.items():\n",
        "        if(key not in new_desc_list.items()):\n",
        "            new_desc_list[key] = list()\n",
        "        # walk through each description for the image\n",
        "        for desc in desc_list:\n",
        "            # encode the sequence\n",
        "            temp = tokenizer.texts_to_sequences([desc])[0]\n",
        "            new_desc_list[key].append(temp)\n",
        "    return new_desc_list\n",
        "\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n9gxRCYfwfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_filename = 'files/dataset.csv'\n",
        "# load descriptions\n",
        "doc = load_doc(csv_filename)\n",
        "\n",
        "\n",
        "# if working on a smaller dataset run this code\n",
        "# with open('train_split.json',\"r\") as f:\n",
        "#     k = f.read()\n",
        "#     x = json.loads(k)\n",
        "# x = x[:1000]\n",
        "\n",
        "# with open('train_split_1000.json','w') as f:\n",
        "#     json.dump(x,f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_imgs = 'files/train_split.json'\n",
        "with open(train_imgs) as f:\n",
        "    k = f.read()\n",
        "    train = json.loads(k)\n",
        "\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc,train)\n",
        "print(\"loaded description\",len(descriptions))\n",
        "\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# save to file\n",
        "desc_filename = 'files/generated_files/descriptions.txt'\n",
        "save_descriptions(descriptions, desc_filename)\n",
        "\n",
        "\n",
        "# descriptions\n",
        "\n",
        "train_descriptions = load_clean_descriptions(desc_filename, train)\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "wrd_list = tokenizer.word_index\n",
        "wrd_list = dict([(value, key) for key, value in wrd_list.items()]) \n",
        "print(\"Vocab size = \",vocab_size)\n",
        "description_dict = create_sequences(tokenizer, train_descriptions)\n",
        "desc_dict_filename = 'files/generated_files/description_dict'\n",
        "save_obj(description_dict, desc_dict_filename)\n",
        "\n",
        "word_list_file = 'files/generated_files/word_list.json'\n",
        "with open(word_list_file,'w') as f:\n",
        "    json.dump(wrd_list,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ARjFuq8fzee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class im2p(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, vec_size, coher_hidden_size, topic_hidden_size, nos_imgfeat, cont_flag, n_layers_cont, n_layers_text, n_layers_couple):\n",
        "        super(im2p,self).__init__()\n",
        "        self.n_layers_cont = n_layers_cont\n",
        "        self.n_layers_text = n_layers_text\n",
        "        self.n_layers_couple = n_layers_couple\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vec_size = vec_size\n",
        "        self.coher_hidden_size = coher_hidden_size\n",
        "        self.topic_hidden_size = topic_hidden_size\n",
        "        self.nos_imgfeat = nos_imgfeat\n",
        "        self.cont_flag = cont_flag\n",
        "        # continue stop network\n",
        "        self.img_encoding = nn.Linear(nos_imgfeat, hidden_size)\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size) # For handling the text inputs\n",
        "        self.gru_cont = nn.GRU(hidden_size, hidden_size, n_layers_cont) # GRU for start stop\n",
        "        self.gru_text = nn.GRU(hidden_size, hidden_size, n_layers_text) # GRU for sentence\n",
        "        self.out_cont = nn.Linear(hidden_size, cont_flag) # Flag indicating if we should continue\n",
        "        self.out_text = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "        # coupling network\n",
        "        self.gru_couple = nn.GRU(vec_size, hidden_size, n_layers_couple) # GRU for the coupling unit\n",
        "        # Coherence Network\n",
        "        self.fc_1_coher = nn.Linear(hidden_size, coher_hidden_size) # First Layer\n",
        "        self.fc_2_coher = nn.Linear(coher_hidden_size, hidden_size) # Second Layer\n",
        "        self.non_lin_coher = nn.SELU(inplace = False)\n",
        "        # Topic Network\n",
        "        self.fc_1_topic = nn.Linear(hidden_size, topic_hidden_size) # First Layer\n",
        "        self.fc_2_topic = nn.Linear(topic_hidden_size, hidden_size) # Second Layer\n",
        "        self.non_lin_topic = nn.SELU(inplace = False)\n",
        "\n",
        "    def forward(self, input, hidden, flag):\n",
        "        if flag == 'level_1': # Passing image features and stars for the first GRU of every sentence - Sentence RNN\n",
        "            ip = self.img_encoding(input) # .view(1, 1, -1)\n",
        "            ip = ip.view(1, 1, -1)\n",
        "            output, hidden = self.gru_cont(ip, hidden)\n",
        "            k = self.out_cont(output[0])\n",
        "            output = self.softmax(k) # Obtain the labels of whether to continue or stop\n",
        "        elif flag == 'level_2': # Passing word embeddings - Word RNN\n",
        "            output = input\n",
        "            if input.size() != torch.Size([1,1,256]):\n",
        "                output = self.embedding(output).view(1, 1, -1)\n",
        "                output = F.relu(output)\n",
        "                output, hidden = self.gru_text(output, hidden)\n",
        "            output, hidden = self.gru_text(output, hidden)\n",
        "            output = self.out_text(output[0])\n",
        "            self.softmax = nn.LogSoftmax()\n",
        "            output = self.softmax(output)\t\n",
        "        elif flag == 'couple': # Forward through the coupling unit\n",
        "            output, hidden = self.gru_couple(input, hidden)\n",
        "        elif flag == 'coher': # Forward through the Coherence Vector Network\n",
        "            output = self.fc_1_coher(input)\n",
        "            output = self.non_lin_coher(output)\n",
        "            output = self.fc_2_coher(output)\n",
        "            output = self.non_lin_coher(output)\n",
        "            hidden = None\n",
        "        elif flag == 'topic': # Forward through the Coherence Vector Network\n",
        "            output = self.fc_1_topic(input)\n",
        "            output = self.non_lin_topic(output)\n",
        "            output = self.fc_2_topic(output)\n",
        "            output = self.non_lin_topic(output)\n",
        "            hidden = None\n",
        "        return output, hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnURHAEnf5oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global USE_CUDA # if to use CUDA\n",
        "USE_CUDA = False\n",
        "global MAX_SENTC # max number of sentences in the paragraphs \n",
        "MAX_SENTC = 7\n",
        "global L_S # learning rate\n",
        "L_S = 5.0\n",
        "global L_W # learning rate\n",
        "L_W = 1.0\n",
        "global  lamb # alpha value in coherencce equation\n",
        "lamb = 0.6\n",
        "global max_length\n",
        "max_length = 10 # to set the length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVXovpO2f7RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraphs = load_obj('files/generated_files/description_dict')\n",
        "\n",
        "pickle_filename = 'files/features.pkl'\n",
        "\n",
        "with open(pickle_filename,'rb') as f:\n",
        "    features = load(f) #load the features dictionary from the pickle file\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eew4zmQmf-VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size_p = 256\n",
        "output_size_p = 7047 # vocab size\n",
        "vec_size_p = 256\n",
        "coher_hidden_size_p = hidden_size_p\n",
        "topic_hidden_size_p = hidden_size_p \n",
        "# all hidden sizes are same\n",
        "nos_imgfeat_p = 2048 # feature vector\n",
        "cont_flag_p = 1\n",
        "n_layers_cont_p = 1 \n",
        "n_layers_text_p = 1 \n",
        "n_layers_couple_p = 1 \n",
        "# criterion_1 = nn.CrossEntropyLoss() # loss function 1\n",
        "criterion_1 = nn.MSELoss()\n",
        "criterion_2 = nn.CrossEntropyLoss() # loss function 2\n",
        "####### OUTPUT_SIZE has been used as per the vocabulary size for 5000 images . Change it according to number of images from preprocessing. #########\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ih74HmggAuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = im2p(hidden_size = hidden_size_p, output_size = output_size_p, vec_size = vec_size_p, coher_hidden_size = coher_hidden_size_p, topic_hidden_size = topic_hidden_size_p, nos_imgfeat = nos_imgfeat_p, cont_flag = cont_flag_p, n_layers_cont = n_layers_cont_p, n_layers_text = n_layers_text_p, n_layers_couple = n_layers_couple_p) # arguments to be passed\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "torch.autograd.set_detect_anomaly(True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6vdQSj3gEjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epochs in range(0,10):\n",
        "    k = 0\n",
        "    loss_epoch_list = []\n",
        "    epoch_timer = timer()\n",
        "    for img_id,p in paragraphs.items():\n",
        "        k=k+1\n",
        "        img_timer = timer()\n",
        "        print(\"epoch = \",epochs,end = ' ')\n",
        "        print(\"image id : \",img_id,end = ' ')\n",
        "        print(\"image_number : \",k)\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        feats = features[img_id]\n",
        "        input_variable = p\n",
        "        target_variable = input_variable # target variable to compare with the output to find loss\n",
        "        model_hidden_st = None # Stores the hidden state vector at every step of the Sentence RNN\n",
        "        nos_sentc = len(input_variable); sent_exec = 0\n",
        "        sent_cand = 0\n",
        "        for st in range(MAX_SENTC): # Iterate to see how many sentences the model intends to generate\n",
        "            \n",
        "            temp_ip = torch.from_numpy(feats)\n",
        "            temp_ip = temp_ip.float()\n",
        "            mod_ip = Variable(temp_ip, requires_grad=True) # Push in the Image Feature Here\n",
        "\n",
        "            if st == 0:\n",
        "                temp_hid = np.zeros(hidden_size_p, dtype = np.float32) # random.uniform(0, 1, (hidden_size - star_embed ) )\n",
        "                temp_hid = temp_hid.reshape(1, 1, hidden_size_p)\n",
        "                model_hidden = Variable(torch.from_numpy(temp_hid), requires_grad=True)\n",
        "            else:\n",
        "                mh = model_hidden_st.cpu().data.detach().numpy()\n",
        "                model_hidden =  Variable(torch.from_numpy( mh[0, 0, :hidden_size_p].reshape(1, 1, hidden_size_p) ), requires_grad=True)\n",
        "\n",
        "            # Check if Variable should be moved to GPU\n",
        "            if USE_CUDA:\n",
        "                mod_ip = mod_ip.cuda()\n",
        "                model_hidden = model_hidden.cuda()\n",
        "\n",
        "            # Call the model for the first time at the beginning of a sentence\n",
        "            output_contstop, model_hidden = model(mod_ip, model_hidden, 'level_1') # Indicating that the first level RNN is to be used\n",
        "            model_hidden_st = model_hidden\n",
        "            strtstp_topv, strtstp_topi = output_contstop.data.topk(1)\n",
        "            strtstp_ni = strtstp_topi[0][0]\n",
        "\n",
        "            if strtstp_ni == 0: # So we continue\n",
        "                sent_cand = sent_cand + 1\n",
        "        sent_cand_temp = torch.tensor(float(sent_cand))#,requires_grad = True)\n",
        "        nos_sentc_temp = torch.tensor(float(nos_sentc))#,requires_grad = True)\n",
        "        loss = loss + L_S * criterion_1(sent_cand_temp,nos_sentc_temp) # The cross-entropy loss over the number of sentences\n",
        "      \n",
        "        val_sent = nos_sentc\n",
        "        # Create the array of topic vectors and construct the Global Topic Vector - Topic Generation Net\n",
        "        gl_mh = np.zeros((1, 1, hidden_size_p, val_sent))\n",
        "        model_hidden_st = None\n",
        "        # Stack up the vectors\n",
        "        for st in range(nos_sentc): # Iterate over each sentence separately\n",
        "            if len(input_variable[st]) <= 1: # If the sentence is of unit length, skip it\n",
        "                continue\n",
        "            \n",
        "            temp_ip = torch.from_numpy(feats)\n",
        "            temp_ip = temp_ip.float()\n",
        "            mod_ip = Variable(temp_ip, requires_grad=True)\n",
        "\n",
        "            if sent_exec == 0: # The first sentence\n",
        "                temp_hid = np.zeros(hidden_size_p, dtype = np.float32) # random.uniform(0, 1, (hidden_size) )\n",
        "                temp_hid = temp_hid.reshape(1, 1, hidden_size_p)\n",
        "                model_hidden = Variable(torch.from_numpy(temp_hid), requires_grad=True) # Push in the Image Feature Here #encoder_hidden\n",
        "                sent_exec = sent_exec + 1\n",
        "            else: # All other sentences are initialized from previous sentences\n",
        "                mh = model_hidden_st.cpu().data.detach().numpy()\n",
        "                model_hidden =  Variable(torch.from_numpy( mh[0, 0, :hidden_size_p].reshape(1, 1, hidden_size_p) ), requires_grad=True) # Obtain the hidden state from the previous hidden state\n",
        "                sent_exec = sent_exec + 1\n",
        "\n",
        "            # Check if Variable should be moved to GPU\n",
        "            if USE_CUDA:\n",
        "                mod_ip = mod_ip.cuda()\n",
        "                model_hidden = model_hidden.cuda()\n",
        "\n",
        "            output_contstop, model_hidden = model(mod_ip, model_hidden, 'level_1') # level_1 indicates that we are using the Senetence RNN\n",
        "            model_hidden_st = model_hidden\n",
        "            gl_mh[0, 0, :, sent_exec-1] = (model(model_hidden_st, None, 'topic')[0].cpu().data.detach().numpy()).reshape(1, 1, hidden_size_p) # Transform the hidden state to obtain the topic vector\n",
        "        \n",
        "        # Compute the global topic vector as a weighted average of the individual topic vectors\n",
        "        glob_vec = gl_mh[0, 0, :, 0].reshape(1, 1, hidden_size_p)\n",
        "        for i in range(1, val_sent):\n",
        "            glob_vec[:, :, :] = glob_vec[:, :, :].copy() + gl_mh[:, :, :, i].reshape(1, 1, hidden_size_p) * (LA.norm(gl_mh[:, :, :, i].reshape(-1)) / np.sum(LA.norm(gl_mh[:, :, :, :].reshape(-1, val_sent).T, axis=1)))\n",
        "\n",
        "\n",
        "        # Process the Sentence RNN\n",
        "        #Previous Hidden State Vector - The Coherence Vector\n",
        "        prev_vec = ( np.zeros((1, 1, hidden_size_p)) ).astype(np.float32)\n",
        "\n",
        "        for st in range(nos_sentc): # Iterate over each sentence separately\n",
        "            \n",
        "            if len(input_variable[st]) <= 1: # If the sentence is of unit length, skip it\n",
        "                continue\n",
        "            ip_var = Variable(torch.LongTensor(input_variable[st]))#, requires_grad=True) # One sentence\n",
        "            op_var = Variable(torch.LongTensor(target_variable[st]))#, requires_grad=True)\n",
        "            input_length = ip_var.size()[0]\n",
        "            target_length = op_var.size()[0]\n",
        "            \n",
        "            loc_vec = (gl_mh[:, :, :, st]).reshape(1, 1, -1) # The original topic vector for the current sentence\n",
        "            comb = np.add((1 - lamb) * loc_vec[0, 0, :], (lamb) * prev_vec[0, 0, :]) # Combine the current topic vector and the coherence vector from the previous sentence\n",
        "            glob_vec = glob_vec.astype(np.float32)\n",
        "            if type(comb) is not np.ndarray:\n",
        "                foo = comb.numpy()\n",
        "                comb = foo\n",
        "            comb = comb.astype(np.float32)\n",
        "            mh = (((model(torch.tensor([[glob_vec[0, 0,:]]]), torch.tensor([[comb]]), 'couple' )[0]).reshape(1, 1, -1)).detach().numpy()).astype(np.float32) # Coupling Unit\n",
        "            \n",
        "            # Construct the input for the first word of a sentence in the Sentence RNN\n",
        "            model_input =  Variable(torch.from_numpy(mh[0, 0, :]), requires_grad=True).reshape(1,1,256)\n",
        "            model_hidden = Variable(torch.from_numpy(temp_hid), requires_grad=True).reshape(1,1,256)\n",
        "\n",
        "            #print(\"model_input\",model_input)\n",
        "            if USE_CUDA:\n",
        "                model_hidden = model_hidden.cuda()\n",
        "                model_input = model_input.cuda()\n",
        "                ip_var = ip_var.cuda()\n",
        "                op_var = op_var.cuda()\n",
        "                \n",
        "            # Teacher forcing: Feed the target as the next input\n",
        "            for di in range(1, target_length, 1):\n",
        "                model_output, model_hidden = model(model_input, model_hidden, 'level_2') # level_2 indicates that we want to use the Sentence RNN\n",
        "                foo = loss\n",
        "                loss = foo + L_W * criterion_2(model_output, op_var[di:di+1]) # Use the second cross-entropy term\n",
        "                \n",
        "                model_input = op_var[di:di + 1]\n",
        "                \t\t\t\n",
        "            # Re-initialize the previous vector\n",
        "            prev_vec = model(model_hidden, None, 'coher')[0].detach()\n",
        "           \n",
        "        # optimizer to be added\t\n",
        "        print(\"loss = \",loss,end = ' ')\n",
        "#         loss_list[epochs].append(loss.data.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch_list.append(loss.data.item())\n",
        "        print(\"time taken for this image =\",timer() - img_timer)\n",
        "        print(\"this epoch =\",timer() - epoch_timer)\n",
        "        \n",
        "    filepath = 'files/models/model' + str(epochs) + '.pth'\n",
        "    torch.save(model,filepath)\n",
        "    \n",
        "    filename = 'files/losses/loss_epoch_' + str(epochs) + '.json'\n",
        "    with open(filename,'w') as f:\n",
        "        json.dump(loss_epoch_list,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJJmO11IgFYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('model_x.pth') # x is the epoch number\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rURPes_dggA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('final/files/generated_files/word_list.json',\"r\") as f:\n",
        "    k = f.read()\n",
        "    wrd_list = json.loads(k)\n",
        "wrd_list = dict([(int(key),value) for key, value in wrd_list.items()]) \n",
        "pickle_filename = 'final/files/features_test.pkl'\n",
        "f = open(pickle_filename,'rb')\n",
        "features = load(f) #load the features dictionary from the pickle file\n",
        "f.close()\n",
        "# img_id = '2406949'\n",
        "img_id = sys.argv[1].split('.')[0]\n",
        "model_hidden_st = None # Stores the hidden state vector at every step of the Sentence RNN\n",
        "pred_words = [] # Stores the list of synthesized words\n",
        "\n",
        "# Create the array of topic vectors and the Global Topic Vector -- Topic Generation Net\n",
        "gl_mh = np.zeros((1, 1, hidden_size_p, MAX_SENTC)); val_sent = 0;\n",
        "\n",
        "for st in range(MAX_SENTC): # Iterate over each sentence separately\n",
        "\n",
        "    feats = features[img_id] \n",
        "\n",
        "\n",
        "    temp_ip = torch.from_numpy(feats)\n",
        "    temp_ip = temp_ip.float()\n",
        "    mod_ip = Variable(temp_ip) # Push in the Image Feature Here\n",
        "\n",
        "    if st == 0: # Initialize the hidden state for the first se\n",
        "        temp_hid = np.zeros(hidden_size_p, dtype = np.float32) # random.uniform(0, 1, (opt.hidden_size - star_embed ) )\n",
        "        temp_hid = temp_hid.reshape(1, 1, hidden_size_p )\n",
        "        model_hidden = Variable(torch.from_numpy(temp_hid))\n",
        "    else:\n",
        "        mh = model_hidden_st.cpu().data.numpy()\n",
        "        model_hidden =  Variable(torch.from_numpy(mh[0, 0, :hidden_size_p].reshape(1, 1, hidden_size_p)))\n",
        "\n",
        "    # Check if Variable should be moved to GPU\n",
        "    if USE_CUDA:\n",
        "        mod_ip = mod_ip.cuda()\n",
        "        model_hidden = model_hidden.cuda()\n",
        "\n",
        "    output_contstop, model_hidden = model(mod_ip,model_hidden,'level_1') # level_1 indicates that we are using the Senetence RNN\n",
        "    model_hidden_st = model_hidden\n",
        "    strtstp_topv, strtstp_topi = output_contstop.data.topk(1)\n",
        "    strtstp_ni = strtstp_topi[0][0]\n",
        "\n",
        "    if strtstp_ni == 0: # So we continue\n",
        "        val_sent = val_sent + 1\n",
        "        gl_mh[0, 0, :, st] = (model(model_hidden_st, None, 'topic')[0].cpu().data.numpy()).reshape(1, 1, hidden_size_p) # Transform the hidden state to obtain the topic vector\n",
        "\n",
        "# Compute the Global Topic Vector as a weighted average of the individual topic vectors\n",
        "glob_vec = gl_mh[0, 0, :, 0].reshape(1, 1, hidden_size_p)\n",
        "for i in range(1, val_sent):\n",
        "    glob_vec[:, :, :] += gl_mh[:, :, :, i].reshape(1, 1, hidden_size_p) * (LA.norm(gl_mh[:, :, :, i].reshape(-1)) / np.sum(LA.norm(gl_mh[:, :, :, :].reshape(-1, val_sent).T, axis=1)))\n",
        "\n",
        "# Sentence Generation Net\n",
        "#Previous Hidden State Vector\n",
        "prev_vec = (np.zeros((1, 1, hidden_size_p))).astype(np.float32)\n",
        "\n",
        "for st in range(MAX_SENTC): # Iterate over each sentence separately and generate the words\n",
        "\n",
        "    sentence = []\n",
        "    loc_vec = (gl_mh[:, :, :, st]).reshape(1, 1, -1) # The original topic vector for the current sentence\n",
        "    comb = np.add((1-lamb) * loc_vec[0, 0, :], (lamb) * prev_vec[0, 0, :]) # Combine the current topic vector and the coherence vector from the previous sentence\n",
        "    if type(comb) is not np.ndarray:\t\t\n",
        "        foo = comb.numpy()\n",
        "        comb = foo\t\n",
        "    comb = comb.astype(np.float32)\n",
        "    glob_vec = glob_vec.astype(np.float32)\t\n",
        "    mh = (((model(torch.tensor([[glob_vec[0, 0,:]]]), torch.tensor([[comb]]), 'couple' )[0]).reshape(1, 1, -1)).detach().numpy()).astype(np.float32)\t\n",
        "\n",
        "    # Construct the input for the first word of a sentence in the Sentence RNN\n",
        "\n",
        "    model_input =  Variable(torch.from_numpy(mh[0, 0, :]), requires_grad=True).reshape(1,1,hidden_size_p)\n",
        "    model_hidden = Variable(torch.from_numpy(temp_hid), requires_grad=True).reshape(1,1,hidden_size_p)\n",
        "\n",
        "    if USE_CUDA:\n",
        "        model_hidden = model_hidden.cuda()\n",
        "        model_input = model_input.cuda()\n",
        "\n",
        "    for di in range(max_length):\n",
        "\n",
        "        model_output, model_hidden = model(model_input, model_hidden, 'level_2') # level_2 indicates that we want to use the Sentence RNN\n",
        "        topv, topi = model_output.data.topk(1) # Standard RNN decoding of the words\n",
        "        ni = topi[0][0]\n",
        "        ni = ni.data.item()\n",
        "        # Check if EOS has been predicted\n",
        "        if ni == len(wrd_list):\n",
        "            sentence.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            sentence.append(wrd_list[ni])\n",
        "            model_input = Variable(torch.LongTensor([ni]))\n",
        "    if sentence not in pred_words:\n",
        "        pred_words.append(sentence)\n",
        "    # Re-initialize the previous vector\n",
        "    prev_vec = model(model_hidden, None, 'coher')[0].detach()\n",
        "sentences = [' '.join(i) for i in pred_words]\n",
        "paragraphs = '.'.join(sentences)\n",
        "print(paragraphs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLC9_EOWghRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_scores1 = []\n",
        "all_scores2 = []\n",
        "all_scores3 = []\n",
        "all_scores4 = []\n",
        "\n",
        "image_no = 0\n",
        "for img_id,reference in descriptions.items(): \n",
        "    model_hidden_st = None # Stores the hidden state vector at every step of the Sentence RNN\n",
        "    pred_words = [] # Stores the list of synthesized words\n",
        "\n",
        "    # Create the array of topic vectors and the Global Topic Vector -- Topic Generation Net\n",
        "    gl_mh = np.zeros((1, 1, hidden_size_p, MAX_SENTC)); val_sent = 0;\n",
        "\n",
        "    for st in range(MAX_SENTC): # Iterate over each sentence separately\n",
        "\n",
        "        feats = features[img_id] \n",
        "\n",
        "\n",
        "        temp_ip = torch.from_numpy(feats)\n",
        "        temp_ip = temp_ip.float()\n",
        "        mod_ip = Variable(temp_ip) # Push in the Image Feature Here\n",
        "\n",
        "        if st == 0: # Initialize the hidden state for the first se\n",
        "            temp_hid = np.zeros(hidden_size_p, dtype = np.float32) # random.uniform(0, 1, (opt.hidden_size - star_embed ) )\n",
        "            temp_hid = temp_hid.reshape(1, 1, hidden_size_p )\n",
        "            model_hidden = Variable(torch.from_numpy(temp_hid))\n",
        "        else:\n",
        "            mh = model_hidden_st.cpu().data.numpy()\n",
        "            model_hidden =  Variable(torch.from_numpy(mh[0, 0, :hidden_size_p].reshape(1, 1, hidden_size_p)))\n",
        "\n",
        "        # Check if Variable should be moved to GPU\n",
        "        if USE_CUDA:\n",
        "            mod_ip = mod_ip.cuda()\n",
        "            model_hidden = model_hidden.cuda()\n",
        "\n",
        "        output_contstop, model_hidden = model(mod_ip,model_hidden,'level_1') # level_1 indicates that we are using the Senetence RNN\n",
        "        model_hidden_st = model_hidden\n",
        "        strtstp_topv, strtstp_topi = output_contstop.data.topk(1)\n",
        "        strtstp_ni = strtstp_topi[0][0]\n",
        "\n",
        "        if strtstp_ni == 0: # So we continue\n",
        "            val_sent = val_sent + 1\n",
        "            gl_mh[0, 0, :, st] = (model(model_hidden_st, None, 'topic')[0].cpu().data.numpy()).reshape(1, 1, hidden_size_p) # Transform the hidden state to obtain the topic vector\n",
        "\n",
        "    # Compute the Global Topic Vector as a weighted average of the individual topic vectors\n",
        "    glob_vec = gl_mh[0, 0, :, 0].reshape(1, 1, hidden_size_p)\n",
        "    for i in range(1, val_sent):\n",
        "        glob_vec[:, :, :] += gl_mh[:, :, :, i].reshape(1, 1, hidden_size_p) * (LA.norm(gl_mh[:, :, :, i].reshape(-1)) / np.sum(LA.norm(gl_mh[:, :, :, :].reshape(-1, val_sent).T, axis=1)))\n",
        "\n",
        "    # Sentence Generation Net\n",
        "    #Previous Hidden State Vector\n",
        "    prev_vec = (np.zeros((1, 1, hidden_size_p))).astype(np.float32)\n",
        "\n",
        "    for st in range(MAX_SENTC): # Iterate over each sentence separately and generate the words\n",
        "\n",
        "        sentence = []\n",
        "        loc_vec = (gl_mh[:, :, :, st]).reshape(1, 1, -1) # The original topic vector for the current sentence\n",
        "        comb = np.add((1-lamb) * loc_vec[0, 0, :], (lamb) * prev_vec[0, 0, :]) # Combine the current topic vector and the coherence vector from the previous sentence\n",
        "        if type(comb) is not np.ndarray:\t\t\n",
        "            foo = comb.numpy()\n",
        "            comb = foo\t\n",
        "        comb = comb.astype(np.float32)\n",
        "        glob_vec = glob_vec.astype(np.float32)\t\n",
        "        mh = (((model(torch.tensor([[glob_vec[0, 0,:]]]), torch.tensor([[comb]]), 'couple' )[0]).reshape(1, 1, -1)).detach().numpy()).astype(np.float32)\t\n",
        "\n",
        "        # Construct the input for the first word of a sentence in the Sentence RNN\n",
        "\n",
        "        model_input =  Variable(torch.from_numpy(mh[0, 0, :]), requires_grad=True).reshape(1,1,hidden_size_p)\n",
        "        model_hidden = Variable(torch.from_numpy(temp_hid), requires_grad=True).reshape(1,1,hidden_size_p)\n",
        "\n",
        "        if USE_CUDA:\n",
        "            model_hidden = model_hidden.cuda()\n",
        "            model_input = model_input.cuda()\n",
        "\n",
        "        for di in range(max_length):\n",
        "\n",
        "            model_output, model_hidden = model(model_input, model_hidden, 'level_2') # level_2 indicates that we want to use the Sentence RNN\n",
        "            topv, topi = model_output.data.topk(1) # Standard RNN decoding of the words\n",
        "            ni = topi[0][0]\n",
        "            ni = ni.data.item()\n",
        "            # Check if EOS has been predicted\n",
        "            if ni == len(wrd_list):\n",
        "                sentence.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                sentence.append(wrd_list[ni])\n",
        "                model_input = Variable(torch.LongTensor([ni]))\n",
        "        if sentence not in pred_words:\n",
        "            pred_words.append(sentence)\n",
        "    # Re-initialize the previous vector\n",
        "        prev_vec = model(model_hidden, None, 'coher')[0].detach()\n",
        "    candidate = pred_words \n",
        "    reference = descriptions[img_id]\n",
        "    score1 = 0\n",
        "    score2 = 0\n",
        "    score3 = 0\n",
        "    score4 = 0\n",
        "\n",
        "    for i in range(min(len(reference),len(candidate))):\n",
        "        score1 += sentence_bleu([reference[i].strip().split()], candidate[i],weights=(1,0,0,0))\n",
        "        score2 += sentence_bleu([reference[i].strip().split()], candidate[i],weights=(0,1,0,0))\n",
        "        score3 += sentence_bleu([reference[i].strip().split()], candidate[i],weights=(0,0,1,0))\n",
        "        score4 += sentence_bleu([reference[i].strip().split()], candidate[i],weights=(0,0,0,1))\n",
        "    all_scores1.append(score1)\n",
        "    all_scores2.append(score2)\n",
        "    all_scores3.append(score3)\n",
        "    all_scores4.append(score4)\n",
        "    print(\"Image No.\",image_no)\n",
        "    image_no += 1\n",
        "\n",
        "print(\"BLEU SCORE\",sum(all_scores1)/len(all_scores1))\n",
        "print(\"BLEU SCORE\",sum(all_scores2)/len(all_scores2))\n",
        "print(\"BLEU SCORE\",sum(all_scores3)/len(all_scores3))\n",
        "print(\"BLEU SCORE\",sum(all_scores4)/len(all_scores4))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}